# -*- coding: utf-8 -*-
"""Tutorial_1_Sentence_classification_with_word_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/deepmipt/dp_tutorials/blob/master/Tutorial_1_Sentence_classification_with_word_embeddings.ipynb

# Tutorial 1.  Sentence classification with word embeddings

This tutorial is aimed to make participants familiar with text classification on **DeepPavlov**.

The tutorial has the following **structure**:

1. [Library and requirements installation](#Library-and-requirements-installation)

2. [Dataset downloading](#Dataset-downloading)

3. [Dataset Reader](#Dataset-Reader): [docs link](https://deeppavlov.readthedocs.io/en/latest/apiref/dataset_readers.html)

4. [Dataset Iterator](#Dataset-Iterator): [docs link](https://deeppavlov.readthedocs.io/en/latest/apiref/dataset_iterators.html)

5. [Preprocessor](#Preprocessor): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html)

6. [Tokenizer](#Tokenizer): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html)

7. [GloVe Embedder](#Embedder): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/data_processors.html)
[pre-trained embeddings link](https://deeppavlov.readthedocs.io/en/latest/intro/pretrained_vectors.html)

8. [Vocabulary of classes](#Vocabulary-of-classes)

9. [Keras Classifier](#Classifier): [docs link](https://deeppavlov.readthedocs.io/en/latest/components/classifiers.html)

## Library and requirements installation

Let's install `DeepPavlov` library and dependencies for Keras classification model.
"""

!pip install deeppavlov

!python -m deeppavlov install intents_snips

"""## Dataset downloading.

This tutorial uses dataset Stanford Sentiment Treebank (SST) from [paper](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf).

The dataset contains unlabelled sentences divided to train/dev/test sets, phrases labelled with float sentiment value. Most of the sentences are contained in labelled list of phrases. Therefore, we are going to extract sentences coinciding with labelled phrases, convert their float sentiment to fine-grained (5 classes: very negative, negative, neutral, positive, very positive) and binary classes (negative and positive only), build two classifiers.

Let's download and extract the SST dataset.
"""

from deeppavlov.core.data.utils import download

download("./stanfordSentimentTreebank.zip", source_url="http://files.deeppavlov.ai/datasets/stanfordSentimentTreebank.zip")

!unzip stanfordSentimentTreebank.zip

!ls stanfordSentimentTreebank/

"""## Dataset Reader

DatasetReaders are components for reading datasets from files. DeepPavlov contains several different DatasetReaders, one can use either presented DatasetReader or build his own component. 

The only requirements is the output of **DatasetReader**: 
* output must be a dictionary with three fields "train", "valid" and "test", 
* each dictionary value must be a list of corresponding samples,
* each sample must be a tuple (x, y) where either x, y or both can also be lists of several inputs.

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/dataset_readers.html
"""

from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader

reader = BasicClassificationDatasetReader()
data = reader.read(data_path="./stanfordSentimentTreebank", 
                   train="train_binary.csv", valid="valid_binary.csv", test="test_binary.csv",
                   x="text", y="binary_label")

data.keys()

"""For every samples we store label(s) as list because we don't know whether it is binary, multi-class or multi-label classification."""

data["train"][0]

"""## Dataset Iterator

DatasetIterators are components for iterating over datasets. DeepPavlov contains several different DatasetIterators, one can either use presented iterator or build his own component.

DatasetIterator must have the following methods:
* **gen_batches** - method generates batches of inputs and expected output to train neural networks. Output is a tuple of a batch of inputs and a batch of expected outputs.
* **get_instances** - method gets all data for a selected data type ("train", "valid", "test"). Output is a tuple of all inputs for a data type and all expected outputs for a data type.
* **split** - method merges/splits data of a selected data type from DatasetReader ("train", "valid", "test").

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/dataset_iterators.html
"""

from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator

iterator = BasicClassificationDatasetIterator(data, seed=42, shuffle=True)

for batch in iterator.gen_batches(data_type="train", batch_size=13):
  print(batch)
  break

"""## Preprocessor

We can preprocess text according to our needs. 
Let's define the most simple preprocessor - lower-casing.

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/preprocessors.html
"""

from deeppavlov.models.preprocessors.str_lower import StrLower

preprocessor = StrLower()

preprocessor(["The Rock is destined to be the 21st Century 's new `` Conan ''."])

"""## Tokenizer

We need to tokenize our texts because we are going to use word embeddings.
DeepPavlov contains several different tokenizers, one can choose the most appropriate.

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/tokenizers.html
"""

from deeppavlov.models.tokenizers.nltk_tokenizer import NLTKTokenizer

tokenizer = NLTKTokenizer()

tokenizer(["The Rock is destined to be the 21st Century 's new `` Conan ''."])

"""## Embedder

We are planning to use non-trainable GloVe word embeddings. Let's download file.

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/embedders.html

Now we need to download GloVe embeddings file. One can download from [here](https://nlp.stanford.edu/projects/glove/) but it downloads more than 800 Mb. To save your time, you can download GloVe embeddings file from DeepPavlov (downloads 350 Mb).
"""

from deeppavlov.core.data.utils import download

download("./glove.6B.100d.txt", source_url="http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt")

"""Now we can define GloVeEmbedder. Parameter `pad_zero` which is set to `True` determines whether to pad embedded batch of tokens to the longest sample length."""

from deeppavlov.models.embedders.glove_embedder import GloVeEmbedder

embedder = GloVeEmbedder(load_path="./glove.6B.100d.txt", 
                         pad_zero=True  # means whether to pad up to the longest sample in a batch
                        )

embedder.dim

embedder(tokenizer(preprocessor(["The Rock is destined to be the 21st Century 's new 'Conan'.",
                                 "The Rock is a new 'Conan'."])))

embedder(tokenizer(preprocessor(["The Rock is destined to be the 21st Century 's new 'Conan'.",
                                 "The Rock is a new 'Conan'."]))).shape

"""## Vocabulary of classes

By default, we assume that we have different classes which also can be given as strings. Therefore, we need to convert them to something more appropriate for classifier. For example, neural classifiers always need to get **one-hot** representation of classes. To get one-hot representation we have to collect a dictionary with all the classes appeared (if needed one can add "unknown" class), index class samples and convert to one-hot representation.

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/core/data.html
"""

from deeppavlov.core.data.simple_vocab import SimpleVocabulary

vocab = SimpleVocabulary(save_path="./binary_classes.dict")

iterator.get_instances(data_type="train")

vocab.fit(iterator.get_instances(data_type="train")[1])

list(vocab.items())

vocab(["positive", "positive", "negative"])

vocab([0, 0, 1])

"""**One-hotter**

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/preprocessors.html
"""

from deeppavlov.models.preprocessors.one_hotter import OneHotter

one_hotter = OneHotter(depth=vocab.len, 
                       single_vector=True  # means we want to have one vector per sample
                      )

one_hotter(vocab(["positive", "positive", "negative"]))

"""**Converting from probability to labels**

Neural model not only accepts one-hot classes representation but also returns for every sample vector of probability distribution of classes. Therefore, we need to use some component to convert probability ditribution to label indices. 

`Proba2Labels` component supports three different model:
* if `max_proba` is true, returns indices of the highest probabilities,
* if `confident_threshold` is given, returns indices with probabiltiies higher than threshold,
* if `top_n` is given, returns `top_n` indices with highest probabilities.

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/preprocessors.html
"""

from deeppavlov.models.classifiers.proba2labels import Proba2Labels

prob2labels = Proba2Labels(max_proba=True)

vocab.len

prob2labels([[0.6, 0.4], 
             [0.2, 0.8],
             [0.1, 0.9]])

vocab(prob2labels([[0.6, 0.4], 
                   [0.2, 0.8],
                   [0.1, 0.9]]))

"""## Classifier

DeepPavlov contains several classification components: sklearn classifiers, NNs on Keras, BERT classifier on tensorflow. This tutorial demonstrates how to build Convolutional neural network classifier on Keras. 

`KerasClassificationModel` is a class building Keras classifier where network architecture is built in a separate class method. 

**DOCS:** http://docs.deeppavlov.ai/en/latest/apiref/models/classifiers.html
"""

from keras.layers import Input, Dense, Activation, Dropout, Flatten, GlobalMaxPooling1D
from keras import Model

from deeppavlov.models.classifiers.keras_classification_model import KerasClassificationModel
from deeppavlov.metrics.accuracy import sets_accuracy

model = KerasClassificationModel(
    filters_cnn=256,
    kernel_sizes_cnn=[3,5,7],
    dropout_rate=0.2,
    dense_size=100,
    save_path="./cnn_model_v0", 
    load_path="./cnn_model_v0", 
    embedding_size=embedder.dim,
    n_classes=vocab.len,
    model_name="cnn_model",  # HERE we put our new network-method name
    optimizer="Adam",
    learning_rate=0.001,
    learning_rate_decay=0.001,
    loss="categorical_crossentropy")

# Method `get_instances` returns all the samples of particular data field
x_valid, y_valid = iterator.get_instances(data_type="valid")
# You need to save model only when validation score is higher than previous one.
# This variable will contain the highest accuracy score
best_score = 0.
patience = 2
impatience = 0

# let's train for 10 epochs
for ep in range(10):
    
    for x, y in iterator.gen_batches(batch_size=64, 
                                     data_type="train", shuffle=True):
        x_embed = embedder(tokenizer(preprocessor(x)))
        y_onehot = one_hotter(vocab(y))
        model.train_on_batch(x_embed, y_onehot)
        
    y_valid_pred = model(embedder(tokenizer(preprocessor(x_valid))))
    score = sets_accuracy(y_valid, vocab(prob2labels(y_valid_pred)))
    print("Epochs done: {}. Valid Accuracy: {}".format(ep + 1, score))
    if score > best_score:
        model.save()
        print("New best score. Saving model.")
        best_score = score    
        impatience = 0
    else:
      impatience += 1
      if impatience == patience:
        print("Out of patience. Stop training.")
        break

# Let's look into obtained resulting outputs
print("Text sample: {}".format(x_valid[0]))
print("True label: {}".format(y_valid[0]))
print("Predicted probability distribution: {}".format(dict(zip(vocab.keys(), 
                                                               y_valid_pred[0]))))
print("Predicted label: {}".format(vocab(prob2labels(y_valid_pred))[0]))

"""# Fine-grained classification

Fine-grained labelled dataset corresponds to multi-class classification task with 5 classes.
Still this classification is not multi-label, so you do not need to change anything from binary classifiaction except of network or training parameters.

The **TASK** is to build from scratch fine-grained classifier.
"""



